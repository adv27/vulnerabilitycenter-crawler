import json

import time

from bs4 import BeautifulSoup
from selenium.webdriver import Chrome, DesiredCapabilities
from selenium.webdriver.common.by import By
from selenium.webdriver.remote.webelement import WebElement
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait

url = input('Enter URL: ')

# Desired Capabilities
desired_capabilities = DesiredCapabilities().CHROME
desired_capabilities['marionette'] = True
desired_capabilities['pageLoadStrategy'] = 'none'  # interactive

# Init Chrome
driver = Chrome(desired_capabilities=desired_capabilities)
driver.get(url)

frame = WebDriverWait(driver, 30).until(
    EC.presence_of_element_located((By.CSS_SELECTOR, "#svcframe"))
)
driver.switch_to.frame(frame)

table: WebElement = WebDriverWait(driver, 30).until(
    EC.presence_of_element_located((By.CSS_SELECTOR, "table.GMMNKTXCKDC"))
)
soup = BeautifulSoup(table.get_attribute('innerHTML'), 'html.parser')
rows = soup.find_all('tr')
results = []
for row in rows[1:]:  # skip the header row
    tds = row.find_all('td')
    columns = list(map(lambda td: td.text, tds))
    obj = {
        'skybox_id': columns[0],
        'cve_id': columns[1],
        'vendor': columns[2],
        'severity': columns[3],
        'reported_at': columns[4],
        'modified_at': columns[5],
        'description': columns[6],
    }
    results.append(obj)

# Saving crawled data to file
with open('results.json', 'w+') as f:
    json.dump(results, f, indent=2)
print(json.dumps(results, indent=2))

# Shutdown driver
time.sleep(10)
driver.quit()
